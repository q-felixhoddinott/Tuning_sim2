{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Data Simulation Analysis\n",
        "\n",
        "This notebook demonstrates the basic data simulation functionality with:\n",
        "- ROC curve analysis\n",
        "- Deciles analysis of outcomes by total score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Import data simulation\n",
        "from data_simulation import Scorecard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Investigate loss rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at how many alerts would be missed if using the total_score to discount e.g. 50% of alerts. \n",
        "\n",
        "def loss_rate(df_analysis,reduction_percent=0.5):\n",
        "    rows = int(np.ceil(len(df_analysis)* reduction_percent))\n",
        "    found = int(df_analysis.sort_values(\"total_score\",ascending=False)[0:rows][\"binary_outcome\"].sum())\n",
        "    total = int(df_analysis[\"binary_outcome\"].sum())\n",
        "    return found/total\n",
        "    \n",
        "def loss_by_beta(beta,reduction_percent=0.5,scorecard=None):\n",
        "    if scorecard:\n",
        "        temp_scores = scorecard.generate_binary_outcome(beta=beta)\n",
        "        df_analysis = pd.DataFrame({\n",
        "            'total_score': scorecard.total_scores['total_score'],\n",
        "            'binary_outcome': temp_scores\n",
        "            })\n",
        "    else:\n",
        "        temp_scorecard = Scorecard(\n",
        "            n_rows=5000,\n",
        "            n_features=8,\n",
        "            binary_prevalence=0.10,\n",
        "            random_state=None,\n",
        "            beta=beta\n",
        "        )\n",
        "        df_analysis = pd.DataFrame({\n",
        "                'total_score': temp_scorecard.total_scores['total_score'],\n",
        "                'binary_outcome': temp_scorecard.binary_outcome\n",
        "                })\n",
        "    return(loss_rate(df_analysis,reduction_percent))\n",
        "\n",
        "# calculate and display loss rate for a range of beta values\n",
        "loss_rate_by_beta_df = None\n",
        "for b in [0,0.5,1,2,3,4]:\n",
        "    s = pd.Series([loss_by_beta(beta=b,scorecard=None) for i in range(200)])\n",
        "    _df = pd.DataFrame({\"beta\":[b],\"mean\":[s.mean()],\"std\":[s.std()]})\n",
        "    try:\n",
        "        loss_rate_by_beta_df = pd.concat([loss_rate_by_beta_df,_df])\n",
        "    except NameError:\n",
        "        loss_rate_by_beta_df = _df\n",
        "    loss_rate_by_beta_df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "loss_rate_by_beta_df.style.format({\"beta\":\"{:3.1f}\",\"mean\":\"{:.1%}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "By varying beta from 0 to 4 we can achieve a model which is the same as random, to highly predictive >99.5% of outcomes found in first half of the data. \n",
        "\n",
        "| Beta \t| SARs found in first 50% of file\t| \n",
        "|--- | --- |\n",
        "|\t0\t| 50.1% | \n",
        "|\t0.5\t| 68.1% | \n",
        "|\t1.0\t| 82.1% | \n",
        "|\t2.0\t| 95.1% | \n",
        "|\t3.0\t| 98.8% | \n",
        "|\t4.0\t| 99.7% | \n",
        "\n",
        "This table is much less stable if the same scorecard is used for all simulations, but it's quicker to calculate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Simulated Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create scorecard with simulated data\n",
        "scorecard = Scorecard(\n",
        "    n_rows=5000,\n",
        "    n_features=3,\n",
        "    binary_prevalence=0.20,\n",
        "    random_state=None,\n",
        "    beta=1\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(scorecard.total_scores)} samples\")\n",
        "print(f\"Binary outcome prevalence: {scorecard.binary_outcome.mean():.3f}\")\n",
        "print(f\"Total score range: {scorecard.total_scores['total_score'].min():.2f} - {scorecard.total_scores['total_score'].max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ROC Curve Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROC curve\n",
        "y_true = scorecard.binary_outcome\n",
        "y_scores = scorecard.total_scores['total_score']\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve: Binary Outcome vs Total Score')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC: {roc_auc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deciles Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create deciles analysis\n",
        "df_analysis = pd.DataFrame({\n",
        "    'total_score': scorecard.total_scores['total_score'],\n",
        "    'binary_outcome': scorecard.binary_outcome\n",
        "})\n",
        "\n",
        "# Calculate deciles (handle duplicate edges with duplicates='drop')\n",
        "df_analysis['decile'] = pd.qcut(df_analysis['total_score'], \n",
        "                               q=10, \n",
        "                               labels=False,\n",
        "                               duplicates='drop')\n",
        "# Convert to string labels\n",
        "df_analysis['decile'] = df_analysis['decile'].apply(lambda x: f'D{x+1}')\n",
        "\n",
        "# Calculate outcome rates by decile\n",
        "decile_stats = df_analysis.groupby('decile').agg({\n",
        "    'total_score': ['count', 'mean', 'min', 'max'],\n",
        "    'binary_outcome': ['sum', 'mean']\n",
        "}).round(4)\n",
        "\n",
        "# Flatten column names\n",
        "decile_stats.columns = ['count', 'avg_score', 'min_score', 'max_score', 'positive_outcomes', 'outcome_rate']\n",
        "\n",
        "# Sort by descending average score (highest scoring decile first)\n",
        "decile_stats = decile_stats.sort_values('avg_score', ascending=False)\n",
        "\n",
        "# Add cumulative columns\n",
        "decile_stats['cumulative_total'] = decile_stats['count'].cumsum()\n",
        "decile_stats['cumulative_outcomes'] = decile_stats['positive_outcomes'].cumsum()\n",
        "decile_stats['cumulative_rate'] = (decile_stats['cumulative_outcomes'] / decile_stats['cumulative_total']).round(4)\n",
        "decile_stats['cumulative_total_percent'] = (decile_stats['cumulative_total'] / len(df_analysis)*100).round(4)\n",
        "decile_stats['cumulative_outcomes_percent'] = (decile_stats['cumulative_outcomes'] / df_analysis[\"binary_outcome\"].sum()*100).round(4)\n",
        "\n",
        "print(\"Deciles Analysis:\")\n",
        "display(decile_stats.drop([\"min_score\",\"max_score\"],axis=1))\n",
        "\n",
        "# Plot outcome rates by decile\n",
        "plt.figure(figsize=(6, 4))\n",
        "decile_stats['outcome_rate'].plot(kind='bar', color='steelblue', alpha=0.7)\n",
        "plt.title('Binary Outcome Rate by Total Score Deciles')\n",
        "plt.xlabel('Decile')\n",
        "plt.ylabel('Outcome Rate')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add overall rate line\n",
        "overall_rate = df_analysis['binary_outcome'].mean()\n",
        "plt.axhline(y=overall_rate, color='red', linestyle='--', \n",
        "            label=f'Overall Rate: {overall_rate:.3f}')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Score Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot total score distribution by outcome\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Separate scores by outcome\n",
        "scores_negative = df_analysis[df_analysis['binary_outcome'] == 0]['total_score']\n",
        "scores_positive = df_analysis[df_analysis['binary_outcome'] == 1]['total_score']\n",
        "\n",
        "plt.hist(scores_negative, bins=30, alpha=0.7, label='Negative Outcome', color='lightblue')\n",
        "plt.hist(scores_positive, bins=30, alpha=0.7, label='Positive Outcome', color='orange')\n",
        "\n",
        "plt.xlabel('Total Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Total Score Distribution by Binary Outcome')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean score for negative outcomes: {scores_negative.mean():.3f}\")\n",
        "print(f\"Mean score for positive outcomes: {scores_positive.mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Tuning_sim2 (UV)",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}